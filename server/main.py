#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Chrome Plus V2.1.1 - åç«¯æœåŠ¡
æ”¯æŒWebSocketå®æ—¶é€šä¿¡ã€Celeryå¼‚æ­¥ä»»åŠ¡å¤„ç†å’ŒRedisæ¶ˆæ¯é˜Ÿåˆ—
"""

from pathlib import Path
import os
import uuid
import asyncio
import json
import logging
from typing import Optional, Dict, Any, List
from contextlib import asynccontextmanager
import datetime
import difflib
import re
import shutil
import zipfile
import tarfile
import httpx
import ast

# ç¯å¢ƒå’Œé…ç½®
from dotenv import load_dotenv

# FastAPIå’ŒWebSocket
from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# Redisï¼ˆå¯é€‰ä¾èµ–ï¼‰
try:
    import redis.asyncio as redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    redis = None

# AIæ¨¡å— (å¯é€‰ï¼Œç”¨äºå…¼å®¹æ€§)
try:
    from pydantic_ai import Agent
    from pydantic_ai.models.openai import OpenAIModel
    from pydantic_ai.providers.openai import OpenAIProvider
    from pydantic_ai.messages import ModelMessage
    PYDANTIC_AI_AVAILABLE = True
except ImportError:
    PYDANTIC_AI_AVAILABLE = False
    Agent = None
    OpenAIModel = None
    OpenAIProvider = None
    ModelMessage = None

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# æ£€æŸ¥Rediså¯ç”¨æ€§
if not REDIS_AVAILABLE:
    logger.warning("Redisä¸å¯ç”¨ï¼Œå°†ç¦ç”¨Redisç›¸å…³åŠŸèƒ½")

# --- ç¯å¢ƒé…ç½® ---
REDIS_URL = os.getenv('REDIS_URL', 'redis://localhost:6379/0')
ENVIRONMENT = os.getenv('ENVIRONMENT', 'development')
DEBUG = os.getenv('DEBUG', 'false').lower() == 'true'

# AI APIé…ç½®
deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')
tavily_api_key = os.getenv('TAVILY_API_KEY')
if not deepseek_api_key:
    logger.warning("æœªæ‰¾åˆ° DEEPSEEK_API_KEYï¼Œä½¿ç”¨æµ‹è¯•æ¨¡å¼")
if not tavily_api_key:
    logger.warning("æœªæ‰¾åˆ° TAVILY_API_KEYï¼Œç½‘ç»œæœç´¢åŠŸèƒ½å°†ä¸å¯ç”¨")

# Redisè¿æ¥æ± 
redis_pool = None

# WebSocketè¿æ¥ç®¡ç†å™¨
class ConnectionManager:
    """WebSocketè¿æ¥ç®¡ç†å™¨"""

    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}
        self.user_channels: Dict[str, str] = {}  # user_id -> channel_id

    async def connect(self, websocket: WebSocket, user_id: Optional[str] = None) -> str:
        """æ¥å—WebSocketè¿æ¥å¹¶è¿”å›é¢‘é“ID"""
        await websocket.accept()
        channel_id = str(uuid.uuid4())
        self.active_connections[channel_id] = websocket

        if user_id:
            self.user_channels[user_id] = channel_id

        logger.info(f"WebSocketè¿æ¥å»ºç«‹: {channel_id}")
        return channel_id

    def disconnect(self, channel_id: str):
        """æ–­å¼€WebSocketè¿æ¥"""
        if channel_id in self.active_connections:
            del self.active_connections[channel_id]

        # æ¸…ç†ç”¨æˆ·é¢‘é“æ˜ å°„
        for user_id, ch_id in list(self.user_channels.items()):
            if ch_id == channel_id:
                del self.user_channels[user_id]
                break

        logger.info(f"WebSocketè¿æ¥æ–­å¼€: {channel_id}")

    async def send_personal_message(self, message: dict, channel_id: str):
        """å‘é€æ¶ˆæ¯åˆ°ç‰¹å®šé¢‘é“"""
        if channel_id in self.active_connections:
            websocket = self.active_connections[channel_id]
            try:
                await websocket.send_json(message)
            except Exception as e:
                logger.error(f"å‘é€æ¶ˆæ¯å¤±è´¥ {channel_id}: {e}")
                self.disconnect(channel_id)

    async def broadcast(self, message: dict):
        """å¹¿æ’­æ¶ˆæ¯åˆ°æ‰€æœ‰è¿æ¥"""
        disconnected = []
        for channel_id, websocket in self.active_connections.items():
            try:
                await websocket.send_json(message)
            except Exception as e:
                logger.error(f"å¹¿æ’­æ¶ˆæ¯å¤±è´¥ {channel_id}: {e}")
                disconnected.append(channel_id)

        # æ¸…ç†æ–­å¼€çš„è¿æ¥
        for channel_id in disconnected:
            self.disconnect(channel_id)

# å…¨å±€è¿æ¥ç®¡ç†å™¨å®ä¾‹
manager = ConnectionManager()

# Rediså‘å¸ƒ/è®¢é˜…ç›‘å¬å™¨
async def redis_listener():
    """ç›‘å¬Rediså‘å¸ƒ/è®¢é˜…æ¶ˆæ¯å¹¶è½¬å‘åˆ°WebSocket"""
    global redis_pool
    if not redis_pool or not REDIS_AVAILABLE:
        logger.info("Redisä¸å¯ç”¨ï¼Œè·³è¿‡Redisç›‘å¬å™¨")
        return

    pubsub = redis_pool.pubsub()

    try:
        # è®¢é˜…æ‰€æœ‰ç»“æœé¢‘é“
        await pubsub.psubscribe("result:*")
        logger.info("Redisç›‘å¬å™¨å¯åŠ¨ï¼Œè®¢é˜…result:*é¢‘é“")

        async for message in pubsub.listen():
            if message['type'] == 'pmessage':
                try:
                    # è§£æé¢‘é“åè·å–channel_id
                    channel_pattern = message['pattern'].decode('utf-8')
                    channel_name = message['channel'].decode('utf-8')
                    channel_id = channel_name.replace('result:', '')

                    # è§£ææ¶ˆæ¯æ•°æ®
                    data = json.loads(message['data'].decode('utf-8'))

                    # è½¬å‘åˆ°å¯¹åº”çš„WebSocketè¿æ¥
                    await manager.send_personal_message(data, channel_id)
                    logger.info(f"è½¬å‘æ¶ˆæ¯åˆ°é¢‘é“ {channel_id}")

                except Exception as e:
                    logger.error(f"å¤„ç†Redisæ¶ˆæ¯å¤±è´¥: {e}")

    except Exception as e:
        logger.error(f"Redisç›‘å¬å™¨é”™è¯¯: {e}")
    finally:
        await pubsub.unsubscribe()
        await pubsub.close()

# åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†
@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    global redis_pool

    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    logger.info("Chrome Plus V2.0 åç«¯æœåŠ¡å¯åŠ¨ä¸­...")

    try:
        # åˆå§‹åŒ–Redisè¿æ¥æ± ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if REDIS_AVAILABLE:
            try:
                redis_pool = redis.ConnectionPool.from_url(REDIS_URL)
                redis_client = redis.Redis(connection_pool=redis_pool)

                # æµ‹è¯•Redisè¿æ¥
                await redis_client.ping()
                logger.info("Redisè¿æ¥æˆåŠŸ")

                # å¯åŠ¨Redisç›‘å¬å™¨
                redis_task = asyncio.create_task(redis_listener())
            except Exception as e:
                logger.warning(f"Redisè¿æ¥å¤±è´¥ï¼Œå°†ç¦ç”¨RedisåŠŸèƒ½: {e}")
                redis_pool = None
        else:
            logger.info("Redisä¸å¯ç”¨ï¼Œè·³è¿‡Redisåˆå§‹åŒ–")
            redis_pool = None

        logger.info("åç«¯æœåŠ¡å¯åŠ¨å®Œæˆ")

        yield

    except Exception as e:
        logger.error(f"æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
        raise
    finally:
        # å…³é—­æ—¶æ¸…ç†
        logger.info("æ­£åœ¨å…³é—­æœåŠ¡...")

        if 'redis_task' in locals():
            redis_task.cancel()
            try:
                await redis_task
            except asyncio.CancelledError:
                pass

        if redis_pool:
            await redis_pool.disconnect()

        logger.info("æœåŠ¡å·²å…³é—­")

# å…¨å±€åŸºç¡€ç›®å½•
base_dir = Path(__file__).parent.resolve() / "test"
os.makedirs(base_dir, exist_ok=True)

# --- Pydantic æ¨¡å‹å®šä¹‰ ---
class ProxyAuth(BaseModel):
    """ä»£ç†è®¤è¯ä¿¡æ¯"""
    username: str
    password: str

class ProxyConfig(BaseModel):
    """ä»£ç†é…ç½®æ¨¡å‹"""
    enabled: bool = False
    type: str = "http"  # http, https, socks5
    host: str = ""
    port: int = 8080
    auth: Optional[ProxyAuth] = None

class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚æ¨¡å‹"""
    message: str
    proxyConfig: Optional[ProxyConfig] = None

    class Config:
        json_schema_extra = {
            "example": {
                "message": "ä½ å¥½ï¼Œè¯·å¸®æˆ‘åˆ›å»ºä¸€ä¸ªæ–‡ä»¶",
                "proxyConfig": {
                    "enabled": True,
                    "type": "http",
                    "host": "127.0.0.1",
                    "port": 8080,
                    "auth": {
                        "username": "user",
                        "password": "pass"
                    }
                }
            }
        }

class ChatResponse(BaseModel):
    """èŠå¤©å“åº”æ¨¡å‹"""
    response: str

    class Config:
        json_schema_extra = {
            "example": {
                "response": "ä½ å¥½ï¼æˆ‘å¯ä»¥å¸®ä½ åˆ›å»ºæ–‡ä»¶ã€‚è¯·å‘Šè¯‰æˆ‘æ–‡ä»¶åå’Œå†…å®¹ã€‚"
            }
        }

class ErrorResponse(BaseModel):
    """é”™è¯¯å“åº”æ¨¡å‹"""
    error: str

    class Config:
        json_schema_extra = {
            "example": {
                "error": "è¯·æ±‚å¤„ç†å¤±è´¥"
            }
        }

# --- ä»£ç†é…ç½®å‡½æ•° ---
def _get_proxy_url(proxy_config: ProxyConfig) -> Optional[str]:
    """æ ¹æ®ProxyConfigæ„å»ºä»£ç†URL"""
    if proxy_config and proxy_config.enabled and proxy_config.host and proxy_config.port:
        if proxy_config.auth:
            import urllib.parse
            username = urllib.parse.quote(proxy_config.auth.username)
            password = urllib.parse.quote(proxy_config.auth.password)
            return f"{proxy_config.type}://{username}:{password}@{proxy_config.host}:{proxy_config.port}"
        else:
            return f"{proxy_config.type}://{proxy_config.host}:{proxy_config.port}"
    return None

def create_async_http_client_with_proxy(proxy_config: Optional[ProxyConfig] = None) -> httpx.AsyncClient:
    """åˆ›å»ºå¸¦ä»£ç†é…ç½®çš„å¼‚æ­¥HTTPå®¢æˆ·ç«¯"""
    client_kwargs = {
        'timeout': httpx.Timeout(30.0, connect=10.0),
        'limits': httpx.Limits(max_keepalive_connections=5, max_connections=10),
        'follow_redirects': True,
    }
    proxy_url = _get_proxy_url(proxy_config)
    if proxy_url:
        logger.info(f"ä½¿ç”¨ä»£ç†: {proxy_config.type}://{proxy_config.host}:{proxy_config.port}")
        client_kwargs['proxies'] = proxy_url
    return httpx.AsyncClient(**client_kwargs)

def create_sync_http_client_with_proxy(proxy_config: Optional[ProxyConfig] = None) -> httpx.Client:
    """åˆ›å»ºå¸¦ä»£ç†é…ç½®çš„åŒæ­¥HTTPå®¢æˆ·ç«¯"""
    client_kwargs = {
        'timeout': httpx.Timeout(30.0, connect=10.0),
        'limits': httpx.Limits(max_keepalive_connections=5, max_connections=10),
        'follow_redirects': True,
    }
    proxy_url = _get_proxy_url(proxy_config)
    if proxy_url:
        logger.info(f"ä½¿ç”¨ä»£ç†: {proxy_config.type}://{proxy_config.host}:{proxy_config.port}")
        client_kwargs['proxies'] = proxy_url
    return httpx.Client(**client_kwargs)


def create_openai_model_with_proxy(proxy_config: Optional[ProxyConfig] = None):
    """åˆ›å»ºå¸¦ä»£ç†é…ç½®çš„OpenAIæ¨¡å‹"""
    if not deepseek_api_key or not PYDANTIC_AI_AVAILABLE:
        return None

    try:
        http_client = create_async_http_client_with_proxy(proxy_config)
        provider = OpenAIProvider(
            base_url='https://api.deepseek.com',
            api_key=deepseek_api_key,
            http_client=http_client
        )
        return OpenAIModel('deepseek-chat', provider=provider)
    except Exception as e:
        logger.error(f"åˆ›å»ºä»£ç†æ¨¡å‹å¤±è´¥: {e}")
        return None

async def test_proxy_connection(proxy_config: ProxyConfig) -> tuple[bool, str]:
    """æµ‹è¯•ä»£ç†è¿æ¥"""
    client = create_async_http_client_with_proxy(proxy_config)
    try:
        test_url = "https://httpbin.org/ip"
        response = await client.get(test_url, timeout=10.0)
        response.raise_for_status()
        data = response.json()
        return True, f"ä»£ç†è¿æ¥æˆåŠŸï¼ŒIP: {data.get('origin', 'unknown')}"
    except Exception as e:
        return False, f"ä»£ç†è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}"
    finally:
        await client.aclose()

def validate_proxy_config(proxy_config: Optional[ProxyConfig]) -> tuple[bool, str]:
    """éªŒè¯ä»£ç†é…ç½®"""
    if not proxy_config or not proxy_config.enabled:
        return True, ""
    if not proxy_config.host or not proxy_config.host.strip():
        return False, "ä»£ç†åœ°å€ä¸èƒ½ä¸ºç©º"
    if not proxy_config.port or not (1 <= proxy_config.port <= 65535):
        return False, "ä»£ç†ç«¯å£å¿…é¡»åœ¨1-65535ä¹‹é—´"
    if proxy_config.type not in ['http', 'https', 'socks5']:
        return False, f"ä¸æ”¯æŒçš„ä»£ç†ç±»å‹: {proxy_config.type}"
    if proxy_config.auth:
        if not proxy_config.auth.username or not proxy_config.auth.username.strip():
            return False, "ä»£ç†ç”¨æˆ·åä¸èƒ½ä¸ºç©º"
        if not proxy_config.auth.password: # å…è®¸ç©ºå¯†ç 
            return False, "ä»£ç†å¯†ç ä¸èƒ½ä¸ºç©º"
    return True, ""

# --- æ•°æ®æ¨¡å‹ ---
class WebSocketMessage(BaseModel):
    """WebSocketæ¶ˆæ¯æ¨¡å‹"""
    type: str
    data: Dict[str, Any]
    timestamp: Optional[str] = None
    channel_id: Optional[str] = None

class ChatWebSocketRequest(BaseModel):
    """WebSocketèŠå¤©è¯·æ±‚æ¨¡å‹"""
    message: str
    user_id: Optional[str] = None
    proxy_config: Optional[ProxyConfig] = None
    api_config: Optional[Dict[str, Any]] = None

# --- FastAPI åº”ç”¨å®ä¾‹ ---
app = FastAPI(
    title="Chrome Plus V2.1.1 API",
    description="AIæ™ºèƒ½ä½“APIï¼Œæ”¯æŒWebSocketå®æ—¶é€šä¿¡ã€Redisæ¶ˆæ¯é˜Ÿåˆ—ã€æ–‡ä»¶æ“ä½œå’Œç½‘ç»œæœç´¢",
    version="2.1.1",
    lifespan=lifespan
)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["chrome-extension://*", "http://localhost:*", "http://127.0.0.1:*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- å·¥å…·å‡½æ•°ï¼šè·¯å¾„éªŒè¯ (ä¿æŒä¸å˜) ---
def _validate_path(target_path: Path, check_existence=False, expect_dir=False, expect_file=False):
    try:
        if not base_dir.exists() or not base_dir.is_dir():
            return False, f"é”™è¯¯ï¼šåŸºç¡€ç›®å½• '{base_dir}' ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•ã€‚"
        resolved = target_path.resolve()
        resolved_base_dir = base_dir.resolve()
        if not (resolved == resolved_base_dir or \
                str(resolved).startswith(str(resolved_base_dir) + os.sep)):
            return False, f"é”™è¯¯ï¼šè·¯å¾„ '{resolved}' è¶…å‡ºäº†å…è®¸çš„æ“ä½œèŒƒå›´ '{base_dir}'ã€‚"
        if check_existence and not resolved.exists():
            return False, f"é”™è¯¯ï¼šè·¯å¾„ '{target_path}' ä¸å­˜åœ¨ã€‚"
        if resolved.exists():
            if expect_dir and not resolved.is_dir():
                return False, f"é”™è¯¯ï¼šè·¯å¾„ '{target_path}' ä¸æ˜¯ä¸€ä¸ªç›®å½•ã€‚"
            if expect_file and not resolved.is_file():
                return False, f"é”™è¯¯ï¼šè·¯å¾„ '{target_path}' ä¸æ˜¯ä¸€ä¸ªæ–‡ä»¶ã€‚"
        elif expect_dir or expect_file:
             pass # å…è®¸è·¯å¾„åœ¨æ£€æŸ¥æ—¶ä¸å¿…é¡»å­˜åœ¨ï¼Œå¦‚æœåªæ˜¯ä¸ºäº†åç»­åˆ›å»º
        return True, ""
    except Exception as e:
        return False, f"è·¯å¾„éªŒè¯æ—¶å‘ç”Ÿå¼‚å¸¸ï¼š{e}"

# --- æ–‡ä»¶æ“ä½œå·¥å…· (ä¿æŒä¸å˜) ---
def read_file(name: str) -> str:
    print(f"(read_file '{name}')")
    p = base_dir / name
    ok, msg = _validate_path(p, check_existence=True, expect_file=True)
    if not ok: return msg
    try: return p.read_text(encoding='utf-8')
    except Exception as e: return f"è¯»å–æ–‡ä»¶ '{name}' æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}"
def list_files(path: str = ".") -> list[str]:
    print(f"(list_files '{path}')")
    p = (base_dir / path); ok, msg = _validate_path(p, check_existence=True, expect_dir=True)
    if not ok: return [msg]
    resolved_p = p.resolve(); items = []
    for item in sorted(resolved_p.iterdir(), key=lambda x:(x.is_file(), x.name.lower())):
        stat = item.stat(); mtime = datetime.datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S')
        if item.is_dir(): items.append(f"{item.name}/ (ç›®å½•, ---, {mtime})")
        else: items.append(f"{item.name} (æ–‡ä»¶, {stat.st_size} bytes, {mtime})")
    return items or [f"ç›®å½• '{path}' ä¸ºç©ºã€‚"]
def rename_file(name: str, new_name: str) -> str:
    print(f"(rename_file '{name}' -> '{new_name}')"); src_path = base_dir / name; dst_path = base_dir / new_name
    ok_src, msg_src = _validate_path(src_path, check_existence=True)
    if not ok_src: return msg_src
    ok_dst, msg_dst = _validate_path(dst_path, check_existence=False) # Destination may not exist
    if not ok_dst: return msg_dst
    try: dst_path.parent.mkdir(parents=True, exist_ok=True); os.rename(src_path, dst_path); return f"é‡å‘½åæˆåŠŸï¼š'{name}' â†’ '{new_name}'"
    except Exception as e: return f"é‡å‘½åæ–‡ä»¶/ç›®å½•æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}"
def write_file(name: str, content: str, mode: str = 'w') -> str:
    print(f"(write_file '{name}' mode='{mode}')"); p = base_dir / name
    ok, msg = _validate_path(p, check_existence=False)
    if not ok: return msg
    if p.exists() and p.is_dir(): return f"é”™è¯¯ï¼šè·¯å¾„ '{name}' æ˜¯ä¸€ä¸ªç›®å½•ï¼Œæ— æ³•å†™å…¥æ–‡ä»¶ã€‚"
    if mode not in ('w', 'a'): return f"é”™è¯¯ï¼šä¸æ”¯æŒçš„å†™å…¥æ¨¡å¼ '{mode}'ã€‚è¯·ä½¿ç”¨ 'w' æˆ– 'a'ã€‚"
    try:
        p.parent.mkdir(parents=True, exist_ok=True)
        with open(p, mode, encoding='utf-8') as f: f.write(content)
        return f"æˆåŠŸå‘ '{name}' å†™å…¥ {len(content.encode('utf-8'))} å­—èŠ‚ã€‚"
    except Exception as e: return f"å†™å…¥æ–‡ä»¶ '{name}' æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}"
def create_directory(name: str) -> str:
    print(f"(create_directory '{name}')"); p = base_dir / name
    ok, msg = _validate_path(p, check_existence=False)
    if not ok: return msg
    if p.exists(): return f"é”™è¯¯ï¼šè·¯å¾„ '{name}' å·²å­˜åœ¨ã€‚"
    try: p.mkdir(parents=True, exist_ok=False); return f"ç›®å½• '{name}' åˆ›å»ºæˆåŠŸã€‚" # exist_ok=False to error if exists
    except FileExistsError: return f"é”™è¯¯ï¼šè·¯å¾„ '{name}' å·²å­˜åœ¨ã€‚"
    except Exception as e: return f"åˆ›å»ºç›®å½• '{name}' å¤±è´¥ï¼š{e}"
def delete_file(name: str) -> str:
    print(f"(delete_file '{name}')"); p = base_dir / name
    ok, msg = _validate_path(p, check_existence=True, expect_file=True)
    if not ok: return msg
    try: p.unlink(); return f"æ–‡ä»¶ '{name}' åˆ é™¤æˆåŠŸã€‚"
    except Exception as e: return f"åˆ é™¤æ–‡ä»¶ '{name}' æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}"
def pwd() -> str: print("(pwd)"); return f"å½“å‰æ“ä½œç›®å½•é™åˆ¶åœ¨: './{base_dir.name}/'"
def diff_files(f1: str, f2: str) -> str:
    print(f"(diff_files '{f1}' '{f2}')"); path1 = base_dir / f1; path2 = base_dir / f2
    ok1, msg1 = _validate_path(path1, check_existence=True, expect_file=True)
    if not ok1: return msg1
    ok2, msg2 = _validate_path(path2, check_existence=True, expect_file=True)
    if not ok2: return msg2
    try:
        lines1 = path1.read_text(encoding='utf-8').splitlines(keepends=True)
        lines2 = path2.read_text(encoding='utf-8').splitlines(keepends=True)
        diff_result = list(difflib.unified_diff(lines1, lines2, fromfile=f1, tofile=f2, lineterm=''))
        return ''.join(diff_result) or f"æ–‡ä»¶ '{f1}' å’Œ '{f2}' å†…å®¹å®Œå…¨ç›¸åŒã€‚"
    except Exception as e: return f"æ¯”è¾ƒæ–‡ä»¶å·®å¼‚æ—¶å‘ç”Ÿé”™è¯¯: {e}"
def _gen_tree(dir_path: Path, prefix: str, current_depth: int, max_depth: int) -> list[str]:
    if max_depth != -1 and current_depth > max_depth: return []
    lines = [];
    try: entries = sorted(list(dir_path.iterdir()), key=lambda x: (not x.is_dir(), x.name.lower()))
    except PermissionError: return [f"{prefix}â””â”€â”€ [æ— æ³•è®¿é—®]"]
    except Exception as e: return [f"{prefix}â””â”€â”€ [è¯»å–é”™è¯¯: {e}]"]
    for i, entry in enumerate(entries):
        is_last = (i == len(entries) - 1); connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
        lines.append(f"{prefix}{connector}{entry.name}{'/' if entry.is_dir() else ''}")
        if entry.is_dir(): new_prefix = prefix + ("    " if is_last else "â”‚   "); lines.extend(_gen_tree(entry, new_prefix, current_depth + 1, max_depth))
    return lines
def tree(path: str = ".", depth: int = -1) -> str:
    print(f"(tree '{path}' depth={depth})"); target_dir_path_relative = Path(path); target_dir_abs_path = (base_dir / target_dir_path_relative)
    ok, msg = _validate_path(target_dir_abs_path, check_existence=True, expect_dir=True)
    if not ok: return msg
    resolved_target_dir = target_dir_abs_path.resolve()
    if resolved_target_dir == base_dir.resolve(): root_display_name = f"./{base_dir.name}"
    else:
        try: root_display_name = str(resolved_target_dir.relative_to(base_dir.resolve()))
        except ValueError: root_display_name = resolved_target_dir.name # Should not happen due to _validate_path
    output_lines = [f"{root_display_name}/"]
    if depth != 0: output_lines.extend(_gen_tree(resolved_target_dir, "", 1, depth))
    return "\n".join(output_lines)
def find_files(pattern: str, path: str = ".", search_content_regex: Optional[str] = None, case_sensitive: bool = False, recursive: bool = True) -> str:
    print(f"(find_files pattern='{pattern}' path='{path}' content_regex='{search_content_regex}')"); search_root_path = (base_dir / path)
    ok, msg = _validate_path(search_root_path, check_existence=True, expect_dir=True)
    if not ok: return msg
    resolved_search_root = search_root_path.resolve(); glob_func = resolved_search_root.rglob if recursive else resolved_search_root.glob
    try: matched_paths = list(glob_func(pattern))
    except Exception as e: return f"æŸ¥æ‰¾æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}"
    if not matched_paths: return f"åœ¨ '{path}' ç›®å½•åŠå…¶å­ç›®å½•ï¼ˆé€’å½’={recursive}ï¼‰ä¸­æœªæ‰¾åˆ°åŒ¹é…æ¨¡å¼ '{pattern}' çš„æ–‡ä»¶æˆ–ç›®å½•ã€‚"
    output_results = []
    if search_content_regex:
        try: regex_flags = 0 if case_sensitive else re.IGNORECASE; compiled_regex = re.compile(search_content_regex, regex_flags)
        except re.error as e: return f"æä¾›çš„æ­£åˆ™è¡¨è¾¾å¼ '{search_content_regex}' æ— æ•ˆ: {e}"
        for file_path_obj in matched_paths:
            if file_path_obj.is_file(): # Only search content in files
                val_ok, val_msg = _validate_path(file_path_obj, check_existence=True, expect_file=True)
                if not val_ok:
                    output_results.append(f"è·³è¿‡æ— æ•ˆè·¯å¾„ {file_path_obj}: {val_msg}")
                    continue
                try:
                    content = file_path_obj.read_text(encoding='utf-8', errors='ignore')
                    for line_num, line_content in enumerate(content.splitlines()):
                        if compiled_regex.search(line_content):
                            relative_file_path_str = str(file_path_obj.relative_to(base_dir))
                            output_results.append(f"{relative_file_path_str}: ç¬¬ {line_num+1} è¡Œ: {line_content.strip()}")
                except Exception as e:
                    relative_file_path_str = str(file_path_obj.relative_to(base_dir))
                    output_results.append(f"è¯»å–æ–‡ä»¶ {relative_file_path_str} å†…å®¹æ—¶å‡ºé”™: {e}")
        return "\n".join(output_results) or f"åœ¨åŒ¹é…æ¨¡å¼ '{pattern}' çš„æ–‡ä»¶ä¸­æœªæ‰¾åˆ°åŒ…å« '{search_content_regex}' çš„å†…å®¹ã€‚"
    else: return "\n".join(str(p.relative_to(base_dir)) for p in matched_paths)
def replace_in_file(name: str, search_regex: str, replace_string: str, count: int = 0) -> str:
    print(f"(replace_in_file '{name}' regex='{search_regex}' replacement='{replace_string}' count={count})"); file_to_modify = base_dir / name
    ok, msg = _validate_path(file_to_modify, check_existence=True, expect_file=True)
    if not ok: return msg
    try:
        original_content = file_to_modify.read_text(encoding='utf-8')
        new_content, num_replacements = re.subn(search_regex, replace_string, original_content, count=count)
        if num_replacements > 0: file_to_modify.write_text(new_content, encoding='utf-8'); return f"åœ¨æ–‡ä»¶ '{name}' ä¸­æˆåŠŸæ›¿æ¢äº† {num_replacements} å¤„åŒ¹é…ã€‚"
        else: return f"åœ¨æ–‡ä»¶ '{name}' ä¸­æœªæ‰¾åˆ°ä¸æ­£åˆ™è¡¨è¾¾å¼ '{search_regex}' åŒ¹é…çš„å†…å®¹ã€‚"
    except re.error as e: return f"æä¾›çš„æ­£åˆ™è¡¨è¾¾å¼ '{search_regex}' æ— æ•ˆ: {e}"
    except Exception as e: return f"åœ¨æ–‡ä»¶ '{name}' ä¸­è¿›è¡Œæ›¿æ¢æ“ä½œæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}"
def archive_files(archive_name: str, items_to_archive: list[str], archive_format: str = "zip") -> str:
    print(f"(archive_files '{archive_name}' items='{items_to_archive}' format='{archive_format}')"); guessed_format = archive_format.lower()
    if guessed_format == "tar":
        if archive_name.lower().endswith((".tar.gz", ".tgz")): guessed_format = "gztar"
        elif archive_name.lower().endswith((".tar.bz2", ".tbz2")): guessed_format = "bztar"

    final_archive_format = guessed_format; archive_path_full = base_dir / archive_name
    ok_arc_path, msg_arc_path = _validate_path(archive_path_full, check_existence=False)
    if not ok_arc_path: return msg_arc_path
    if archive_path_full.exists(): return f"é”™è¯¯ï¼šå½’æ¡£æ–‡ä»¶ '{archive_name}' å·²å­˜åœ¨ã€‚"

    abs_paths_to_archive = []
    for item_name_str in items_to_archive:
        item_path = base_dir / item_name_str; ok_item, msg_item = _validate_path(item_path, check_existence=True)
        if not ok_item: return f"é”™è¯¯ï¼šè¦å½’æ¡£çš„é¡¹ '{item_name_str}' æ— æ•ˆæˆ–ä¸å­˜åœ¨ï¼š{msg_item}"
        abs_paths_to_archive.append(item_path)
    if not abs_paths_to_archive: return "é”™è¯¯ï¼šæ²¡æœ‰æŒ‡å®šä»»ä½•æœ‰æ•ˆçš„æ–‡ä»¶æˆ–ç›®å½•è¿›è¡Œå½’æ¡£ã€‚"

    valid_formats_map = {"zip": None, "tar": "w", "gztar": "w:gz", "bztar": "w:bz2"}
    if final_archive_format not in valid_formats_map: return f"é”™è¯¯ï¼šä¸æ”¯æŒçš„å½’æ¡£æ ¼å¼ '{final_archive_format}'ã€‚æ”¯æŒçš„æ ¼å¼: {', '.join(valid_formats_map.keys())}."
    try:
        archive_path_full.parent.mkdir(parents=True, exist_ok=True)
        if final_archive_format == "zip":
            with zipfile.ZipFile(archive_path_full, 'w', zipfile.ZIP_DEFLATED) as zf:
                for item_abs_path in abs_paths_to_archive:
                    arcname_in_zip = item_abs_path.relative_to(base_dir)
                    if item_abs_path.is_file(): zf.write(item_abs_path, arcname_in_zip)
                    elif item_abs_path.is_dir():
                        for root, _, files_in_dir in os.walk(item_abs_path):
                            root_path_obj = Path(root)
                            for file_name_in_dir in files_in_dir:
                                file_to_add_path = root_path_obj / file_name_in_dir
                                ok_f, msg_f = _validate_path(file_to_add_path, check_existence=True, expect_file=True)
                                if not ok_f:
                                    print(f"è­¦å‘Š: è·³è¿‡å½’æ¡£ä¸­çš„æ— æ•ˆæ–‡ä»¶ {file_to_add_path}: {msg_f}")
                                    continue
                                file_arcname_in_zip = file_to_add_path.relative_to(base_dir)
                                zf.write(file_to_add_path, file_arcname_in_zip)
        else: # tar based formats
            tar_mode = valid_formats_map[final_archive_format]
            with tarfile.open(archive_path_full, tar_mode) as tf:
                for item_abs_path in abs_paths_to_archive:
                    arcname_in_tar = item_abs_path.relative_to(base_dir)
                    tf.add(item_abs_path, arcname=arcname_in_tar)
        return f"æˆåŠŸåˆ›å»ºå½’æ¡£ '{archive_name}' (æ ¼å¼: {final_archive_format})ã€‚"
    except Exception as e:
        if archive_path_full.exists():
            try: archive_path_full.unlink()
            except: pass
        return f"åˆ›å»ºå½’æ¡£ '{archive_name}' æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}"
def extract_archive(archive_name: str, destination_path: str = ".", specific_members: Optional[list[str]] = None) -> str:
    print(f"(extract_archive '{archive_name}' dest='{destination_path}' members='{specific_members}')"); archive_file_to_extract = base_dir / archive_name
    ok_arc, msg_arc = _validate_path(archive_file_to_extract, check_existence=True, expect_file=True)
    if not ok_arc: return msg_arc

    extraction_dest_dir_relative = Path(destination_path)
    extraction_dest_dir_abs = (base_dir / extraction_dest_dir_relative).resolve()
    
    ok_dest, msg_dest = _validate_path(extraction_dest_dir_abs, check_existence=False, expect_dir=True if extraction_dest_dir_abs.exists() else False)
    if not ok_dest: return msg_dest

    try:
        extraction_dest_dir_abs.mkdir(parents=True, exist_ok=True)
        extracted_count = 0
        actual_extracted_members = []

        if archive_file_to_extract.name.lower().endswith(".zip"):
            with zipfile.ZipFile(archive_file_to_extract, 'r') as zf:
                members_to_extract_from_zip = zf.namelist()
                if specific_members:
                    selected_zip_members = []
                    normalized_zip_member_map = {m.replace("\\", "/"): m for m in members_to_extract_from_zip}
                    for sm_query in specific_members:
                        normalized_sm_query = sm_query.replace("\\", "/")
                        if normalized_sm_query in normalized_zip_member_map:
                            selected_zip_members.append(normalized_zip_member_map[normalized_sm_query])
                        else:
                            dir_sm_query = normalized_sm_query if normalized_sm_query.endswith("/") else normalized_sm_query + "/"
                            found_dir_member = False
                            for zip_m_norm, zip_m_orig in normalized_zip_member_map.items():
                                if zip_m_norm.startswith(dir_sm_query):
                                    selected_zip_members.append(zip_m_orig)
                                    found_dir_member = True
                            if not found_dir_member:
                                print(f"è­¦å‘Šï¼šåœ¨ZIPå½’æ¡£ '{archive_name}' ä¸­æœªæ‰¾åˆ°æˆå‘˜æˆ–ä»¥æ­¤ä¸ºå‰ç¼€çš„æˆå‘˜ '{sm_query}'ã€‚")
                    members_to_extract_from_zip = list(set(selected_zip_members))
                
                if not members_to_extract_from_zip and specific_members:
                    return f"é”™è¯¯ï¼šåœ¨ZIPå½’æ¡£ä¸­æœªæ‰¾åˆ°ä»»ä½•æŒ‡å®šçš„æˆå‘˜è¿›è¡Œè§£å‹ã€‚"

                zf.extractall(path=extraction_dest_dir_abs, members=members_to_extract_from_zip if specific_members else None)
                extracted_count = len(members_to_extract_from_zip if specific_members else zf.namelist())
                actual_extracted_members = members_to_extract_from_zip if specific_members else zf.namelist()

        elif tarfile.is_tarfile(archive_file_to_extract):
            with tarfile.open(archive_file_to_extract, 'r:*') as tf:
                tar_members_to_extract_info = []
                all_tar_members_info = tf.getmembers()

                if specific_members:
                    normalized_tar_member_map = {m.name.replace("\\", "/"): m for m in all_tar_members_info}
                    for sm_name_query in specific_members:
                        normalized_sm_query = sm_name_query.replace("\\", "/")
                        if normalized_sm_query in normalized_tar_member_map:
                            tar_members_to_extract_info.append(normalized_tar_member_map[normalized_sm_query])
                        else:
                            dir_sm_query = normalized_sm_query if normalized_sm_query.endswith("/") else normalized_sm_query + "/"
                            found_dir_member = False
                            for tar_m_norm, tar_m_info in normalized_tar_member_map.items():
                                if tar_m_norm.startswith(dir_sm_query):
                                    tar_members_to_extract_info.append(tar_m_info)
                                    found_dir_member = True
                            if not found_dir_member:
                                print(f"è­¦å‘Šï¼šåœ¨TARå½’æ¡£ '{archive_name}' ä¸­æœªæ‰¾åˆ°æˆå‘˜æˆ–ä»¥æ­¤ä¸ºå‰ç¼€çš„æˆå‘˜ '{sm_name_query}'ã€‚")
                    tar_members_to_extract_info = list(set(tar_members_to_extract_info))
                else:
                    tar_members_to_extract_info = all_tar_members_info
                
                if not tar_members_to_extract_info and specific_members:
                     return f"é”™è¯¯ï¼šåœ¨TARå½’æ¡£ä¸­æœªæ‰¾åˆ°ä»»ä½•æŒ‡å®šçš„æˆå‘˜è¿›è¡Œè§£å‹ã€‚"

                tf.extractall(path=extraction_dest_dir_abs, members=tar_members_to_extract_info if specific_members else None)
                extracted_count = len(tar_members_to_extract_info)
                actual_extracted_members = [m.name for m in tar_members_to_extract_info]
        else:
            return f"é”™è¯¯ï¼šæ— æ³•è¯†åˆ«çš„å½’æ¡£æ–‡ä»¶æ ¼å¼æˆ–æ–‡ä»¶ '{archive_name}' å·²æŸåã€‚"

        display_destination_path = str(extraction_dest_dir_abs.relative_to(base_dir)) if extraction_dest_dir_abs.is_relative_to(base_dir) else str(extraction_dest_dir_abs)

        result_msg = f"ä» '{archive_name}' æˆåŠŸè§£å‹ {extracted_count} ä¸ªæˆå‘˜/æ–‡ä»¶åˆ° './{display_destination_path}'ã€‚"
        if actual_extracted_members:
            result_msg += f"\nè§£å‹çš„æˆå‘˜åˆ—è¡¨ (éƒ¨åˆ†): {', '.join(actual_extracted_members[:10])}{'...' if len(actual_extracted_members) > 10 else ''}"
        return result_msg
    except Exception as e:
        return f"è§£å‹å½’æ¡£ '{archive_name}' æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}"
def backup_file(name: str, backup_dir_name: str = "backups") -> str:
    print(f"(backup_file '{name}' backup_dir='{backup_dir_name}')"); source_file = base_dir / name
    ok_src, msg_src = _validate_path(source_file, check_existence=True, expect_file=True)
    if not ok_src: return msg_src

    backup_target_dir_relative = Path(backup_dir_name)
    backup_target_dir_abs = (base_dir / backup_target_dir_relative).resolve()
    
    ok_dest_dir, msg_dest_dir = _validate_path(backup_target_dir_abs, check_existence=False)
    if not ok_dest_dir: return msg_dest_dir

    try: backup_target_dir_abs.mkdir(parents=True, exist_ok=True)
    except Exception as e: return f"åˆ›å»ºå¤‡ä»½ç›®å½• '{backup_dir_name}' å¤±è´¥: {e}"

    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    backup_filename = f"{source_file.stem}.{timestamp}{source_file.suffix}.bak"
    destination_backup_file_path = backup_target_dir_abs / backup_filename

    ok_dest_file, msg_dest_file = _validate_path(destination_backup_file_path, check_existence=False)
    if not ok_dest_file: return msg_dest_file
    if destination_backup_file_path.exists(): return f"é”™è¯¯ï¼šå¤‡ä»½ç›®æ ‡æ–‡ä»¶ '{destination_backup_file_path.name}' å·²åœ¨ '{backup_dir_name}' ä¸­å­˜åœ¨ã€‚"

    try:
        shutil.copy2(source_file, destination_backup_file_path)
        relative_backup_path_str = str(destination_backup_file_path.relative_to(base_dir))
        return f"æ–‡ä»¶ '{name}' å·²æˆåŠŸå¤‡ä»½åˆ°ï¼š'{relative_backup_path_str}'"
    except Exception as e: return f"å¤‡ä»½æ–‡ä»¶ '{name}' æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}"

def get_system_info() -> str:
    print("(get_system_info)")
    import platform
    import socket
    import psutil
    info = {
        "æ“ä½œç³»ç»Ÿ": platform.system() + " " + platform.release(),
        "ä¸»æœºå": socket.gethostname(),
        "CPUæ ¸å¿ƒæ•°": psutil.cpu_count(),
        "æ€»å†…å­˜(GB)": round(psutil.virtual_memory().total / (1024**3), 2),
        "å½“å‰ç”¨æˆ·": psutil.Process().username()
    }
    return json.dumps(info, ensure_ascii=False, indent=2)

def tavily_search_tool(query: str) -> str:
    """ç½‘ç»œæœç´¢å·¥å…·ï¼Œä½¿ç”¨Tavily APIè¿›è¡Œå®æ—¶æœç´¢ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶"""
    print(f"(tavily_search_tool '{query}')")
    if not tavily_api_key:
        return "é”™è¯¯ï¼šæœªé…ç½®TAVILY_API_KEYï¼Œæ— æ³•è¿›è¡Œç½‘ç»œæœç´¢ã€‚"

    endpoint = "https://api.tavily.com/search"
    headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {tavily_api_key}'}
    data = {'query': query, 'search_depth': 'basic', 'include_answer': True, 'max_results': 5}

    max_retries = 3
    for attempt in range(max_retries):
        try:
            # æ¯æ¬¡é‡è¯•éƒ½åˆ›å»ºæ–°çš„å®¢æˆ·ç«¯å®ä¾‹
            client = create_sync_http_client_with_proxy(None)

            with client:
                response = client.post(endpoint, headers=headers, json=data)
                response.raise_for_status()
                result = response.json()

                if result.get('results'):
                    formatted_results = f"ğŸ” æœç´¢æŸ¥è¯¢: {query}\n\n"
                    if result.get('answer'):
                        formatted_results += f"ğŸ“ ç­”æ¡ˆæ‘˜è¦:\n{result['answer']}\n\n"
                    formatted_results += "ğŸŒ ç›¸å…³é“¾æ¥:\n"
                    for i, item in enumerate(result['results'][:5], 1):
                        title = item.get('title', 'æ— æ ‡é¢˜')
                        url = item.get('url', '')
                        content = item.get('content', '')[:200] + '...' if len(item.get('content', '')) > 200 else item.get('content', '')
                        formatted_results += f"{i}. **{title}**\n   ğŸ”— {url}\n   ğŸ“„ {content}\n\n"
                    return formatted_results
                return f"æœªæ‰¾åˆ°å…³äº '{query}' çš„æœç´¢ç»“æœã€‚"

        except httpx.ConnectError as e:
            if "SSL" in str(e) or "EOF" in str(e):
                logger.warning(f"Tavilyæœç´¢SSLè¿æ¥é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    import time
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                    continue
                else:
                    return f"ç½‘ç»œæœç´¢å¤±è´¥ï¼šSSLè¿æ¥é”™è¯¯ã€‚å»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥ã€‚"
            else:
                return f"ç½‘ç»œæœç´¢å¤±è´¥ï¼šè¿æ¥é”™è¯¯ - {str(e)}"

        except httpx.TimeoutException as e:
            logger.warning(f"Tavilyæœç´¢è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                import time
                time.sleep(1)
                continue
            else:
                return f"ç½‘ç»œæœç´¢å¤±è´¥ï¼šè¯·æ±‚è¶…æ—¶ã€‚"

        except httpx.HTTPStatusError as e:
            return f"ç½‘ç»œæœç´¢å¤±è´¥ï¼šHTTPé”™è¯¯ {e.response.status_code}"

        except Exception as e:
            logger.error(f"Tavilyæœç´¢å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            if attempt < max_retries - 1:
                import time
                time.sleep(1)
                continue
            else:
                return f"ç½‘ç»œæœç´¢å¤±è´¥ï¼š{str(e)}"

    return "ç½‘ç»œæœç´¢å¤±è´¥ï¼šè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°"

# --- ç³»ç»Ÿæç¤º (ä¿®æ”¹å) ---
BASE_SYSTEM_PROMPT = f"""ä½ æ˜¯ ShellAIï¼Œä¸€ä¸ªç»éªŒä¸°å¯Œçš„ç¨‹åºå‘˜åŠ©æ‰‹ï¼Œä½¿ç”¨ä¸­æ–‡ä¸ç”¨æˆ·äº¤æµã€‚
ä½ çš„ä¸»è¦ä»»åŠ¡æ˜¯ååŠ©ç”¨æˆ·è¿›è¡Œæ–‡ä»¶å’Œç›®å½•æ“ä½œï¼Œä»¥åŠåœ¨éœ€è¦æ—¶è¿›è¡Œç½‘ç»œæœç´¢ã€‚
å½“å‰å·¥ä½œç›®å½•ä¸¥æ ¼é™åˆ¶åœ¨ './{base_dir.name}/'ï¼Œæ‰€æœ‰æ–‡ä»¶æ“ä½œéƒ½å°†åœ¨è¿™ä¸ªæ²™ç®±ç›®å½•å†…è¿›è¡Œã€‚

å¯ç”¨å·¥å…·:
- æ–‡ä»¶/ç›®å½•æ“ä½œ (æ‰€æœ‰è·¯å¾„å‚æ•°å‡ç›¸å¯¹äº './{base_dir.name}/'):
  `read_file(name: str)`: è¯»å–æ–‡ä»¶å†…å®¹ã€‚
  `list_files(path: str = ".")`: åˆ—å‡ºç›®å½•å†…å®¹ã€‚
  `rename_file(name: str, new_name: str)`: é‡å‘½åæ–‡ä»¶æˆ–ç›®å½•ã€‚
  `write_file(name: str, content: str, mode: str = 'w')`: å†™å…¥æ–‡ä»¶ (wè¦†ç›–, aè¿½åŠ )ã€‚
  `create_directory(name: str)`: åˆ›å»ºç›®å½•ã€‚
  `delete_file(name: str)`: åˆ é™¤æ–‡ä»¶ (ä¸èƒ½åˆ é™¤ç›®å½•)ã€‚
  `pwd()`: æ˜¾ç¤ºå½“å‰AIæ“ä½œçš„åŸºç¡€ç›®å½•ã€‚
  `diff_files(f1: str, f2: str)`: æ¯”è¾ƒä¸¤ä¸ªæ–‡ä»¶çš„å·®å¼‚ã€‚
  `tree(path: str = ".", depth: int = -1)`: æ ‘çŠ¶æ˜¾ç¤ºç›®å½•ç»“æ„ã€‚
  `find_files(pattern: str, path: str = ".", search_content_regex: str = None, case_sensitive: bool = False, recursive: bool = True)`: æŸ¥æ‰¾æ–‡ä»¶ï¼Œå¯é€‰å†…å®¹æœç´¢ã€‚
  `replace_in_file(name: str, search_regex: str, replace_string: str, count: int = 0)`: æ–‡ä»¶å†…æ­£åˆ™æ›¿æ¢ã€‚
  `archive_files(archive_name: str, items_to_archive: list[str], archive_format: str = "zip")`: å½’æ¡£æ–‡ä»¶æˆ–ç›®å½• (æ”¯æŒ zip, tar, tar.gz/tgz, tar.bz2/tbz2)ã€‚
  `extract_archive(archive_name: str, destination_path: str = ".", specific_members: list[str] = None)`: è§£å‹å½’æ¡£æ–‡ä»¶ã€‚
  `backup_file(name: str, backup_dir_name: str = "backups")`: å¤‡ä»½æ–‡ä»¶ã€‚
  `get_system_info()`: è·å–æœ¬æœºç³»ç»Ÿä¿¡æ¯ã€‚
- ç½‘ç»œæœç´¢:
  `tavily_search_tool(query: str)`: å½“ä½ éœ€è¦æŸ¥æ‰¾å½“å‰çŸ¥è¯†åº“ä¹‹å¤–çš„ä¿¡æ¯ã€å®æ—¶ä¿¡æ¯æˆ–è¿›è¡Œå¹¿æ³›çš„ç½‘ç»œæœç´¢æ—¶ä½¿ç”¨æ­¤å·¥å…·ã€‚

# <<< æ ¸å¿ƒä¿®æ”¹åŒºåŸŸï¼šç”¨æˆ·äº¤äº’æŒ‡å— >>>
ç”¨æˆ·äº¤äº’æŒ‡å—:
- **é¦–è¦åŸåˆ™**: ä»”ç»†ç†è§£ç”¨æˆ·æ„å›¾ã€‚åŒºåˆ†ç”¨æˆ·æ˜¯åœ¨è¿›è¡Œæ™®é€šå¯¹è¯ï¼Œè¿˜æ˜¯åœ¨ä¸‹è¾¾éœ€è¦ä½¿ç”¨å·¥å…·çš„æ˜ç¡®æŒ‡ä»¤ã€‚
- **ä½•æ—¶ç›´æ¥å›ç­” (ä¸ä½¿ç”¨å·¥å…·)**:
  - å½“ç”¨æˆ·è¿›è¡Œé—®å€™ï¼ˆå¦‚â€œä½ å¥½â€ï¼‰ã€æ„Ÿè°¢æˆ–è¿›è¡Œç®€å•çš„æ—¥å¸¸å¯¹è¯æ—¶ï¼Œè¯·åƒä¸€ä¸ªåŠ©æ‰‹ä¸€æ ·ç”¨è‡ªç„¶è¯­è¨€å›å¤ã€‚
  - å½“ç”¨æˆ·è¯¢é—®ä½ çš„èº«ä»½ã€èƒ½åŠ›æˆ–å¯ç”¨å·¥å…·ï¼ˆå¦‚â€œä½ æ˜¯è°â€ã€â€œä½ èƒ½åšä»€ä¹ˆâ€ã€â€œä½ æœ‰ä»€ä¹ˆå·¥å…·â€ï¼‰æ—¶ï¼Œè¯·æ ¹æ®æœ¬æç¤ºä¸­çš„ä¿¡æ¯ç›´æ¥å›ç­”ï¼Œä¸è¦è°ƒç”¨å·¥å…·ã€‚
  - ä¾‹å¦‚ï¼Œå¦‚æœç”¨æˆ·é—®â€œä½ æœ‰ä»€ä¹ˆå·¥å…·â€ï¼Œä½ åº”è¯¥å›ç­”ï¼šâ€œæˆ‘å¯ç”¨çš„å·¥å…·æœ‰æ–‡ä»¶æ“ä½œç±»çš„ï¼ˆå¦‚è¯»å†™ã€åˆ—å‡ºã€é‡å‘½åæ–‡ä»¶ç­‰ï¼‰å’Œç½‘ç»œæœç´¢ç±»çš„...â€ï¼Œè€Œä¸æ˜¯è°ƒç”¨`list_files()`ã€‚
- **ä½•æ—¶ä½¿ç”¨å·¥å…·**:
  - ä»…å½“ç”¨æˆ·çš„è¯·æ±‚æ˜¯ä¸€ä¸ª**æ˜ç¡®çš„ã€å¯æ‰§è¡Œçš„ä»»åŠ¡**ï¼Œä¸”è¯¥ä»»åŠ¡ä¸ä¸Šè¿°æŸä¸ªå·¥å…·çš„åŠŸèƒ½å®Œå…¨åŒ¹é…æ—¶ï¼Œæ‰è°ƒç”¨å·¥å…·ã€‚
  - ä¾‹å¦‚ï¼šâ€œåˆ›å»ºä¸€ä¸ªåä¸º'a.txt'çš„æ–‡ä»¶â€ã€â€œåˆ—å‡ºå½“å‰ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶â€ã€â€œæœç´¢ä¸€ä¸‹ä»Šå¤©çš„å¤©æ°”â€ã€‚
  - **å·¥å…·è°ƒç”¨æ ¼å¼**: å½“ä½ å†³å®šä½¿ç”¨å·¥å…·æ—¶ï¼Œä½ çš„å›å¤**å¿…é¡»ä¸”åªèƒ½**æ˜¯ä¸€è¡ŒPythonä»£ç ï¼Œå³å‡½æ•°è°ƒç”¨æœ¬èº«ï¼Œä¾‹å¦‚ï¼š`write_file("example.txt", "hello world")`ã€‚ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–\`\`\`æ ‡è®°ã€‚
- **æ“ä½œåæŠ¥å‘Š**: åœ¨å·¥å…·æ‰§è¡Œåï¼Œä½ ä¼šæ”¶åˆ°ç»“æœã€‚è¯·æ ¹æ®è¯¥ç»“æœå‘ç”¨æˆ·æŠ¥å‘Šæ“ä½œçš„æˆåŠŸä¸å¦ã€‚å¦‚æœå¤±è´¥ï¼Œè¯·è§£é‡ŠåŸå› ã€‚
"""

def create_intelligent_agent(proxy_config: Optional[Dict] = None):
    """åˆ›å»ºæ™ºèƒ½ä½“å®ä¾‹"""
    return {
        'proxy_config': proxy_config,
        'tools': {
            'read_file': read_file,
            'list_files': list_files,
            'write_file': write_file,
            'create_directory': create_directory,
            'delete_file': delete_file,
            'pwd': pwd,
            'get_system_info': get_system_info,
            'tavily_search_tool': tavily_search_tool,
            'rename_file': rename_file,
            'diff_files': diff_files,
            'tree': tree,
            'find_files': find_files,
            'replace_in_file': replace_in_file,
            'archive_files': archive_files,
            'extract_archive': extract_archive,
            'backup_file': backup_file
        },
        'system_prompt': BASE_SYSTEM_PROMPT
    }

def _call_deepseek_api(prompt: str, proxy_config: Optional[Dict] = None) -> str:
    """è°ƒç”¨DeepSeek APIï¼ŒåŒ…å«SSLé”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶"""
    if not deepseek_api_key:
        # å¦‚æœæ²¡æœ‰API Keyï¼Œæ¨¡æ‹Ÿä¸€ä¸ªå¯¹è¯å¼çš„å›å¤
        if "ä½ å¥½" in prompt or "ä½  å¥½" in prompt:
             return "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"
        return "æœªé…ç½®DEEPSEEK_API_KEYï¼Œå½“å‰ä¸ºæµ‹è¯•æ¨¡å¼ã€‚"

    endpoint = "https://api.deepseek.com/v1/chat/completions"
    headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {deepseek_api_key}'}
    data = {'model': 'deepseek-chat', 'messages': [{'role': 'user', 'content': prompt}], 'stream': False, 'temperature': 0.1, 'max_tokens': 4000}

    proxy_obj = ProxyConfig(**proxy_config) if proxy_config else None

    max_retries = 3
    for attempt in range(max_retries):
        try:
            # æ¯æ¬¡é‡è¯•éƒ½åˆ›å»ºæ–°çš„å®¢æˆ·ç«¯å®ä¾‹
            client = create_sync_http_client_with_proxy(proxy_obj)

            with client:
                response = client.post(endpoint, headers=headers, json=data)
                response.raise_for_status()
                result = response.json()

                # æ£€æŸ¥å“åº”æ ¼å¼
                if not result.get('choices'):
                    raise Exception(f"APIå“åº”ç¼ºå°‘choiceså­—æ®µ: {result}")

                if not isinstance(result['choices'], list) or len(result['choices']) == 0:
                    raise Exception(f"APIå“åº”choiceså­—æ®µæ ¼å¼é”™è¯¯: {result['choices']}")

                first_choice = result['choices'][0]
                if not first_choice.get('message'):
                    raise Exception(f"APIå“åº”ç¼ºå°‘messageå­—æ®µ: {first_choice}")

                message_content = first_choice['message'].get('content')
                if not message_content:
                    raise Exception(f"APIå“åº”messageå†…å®¹ä¸ºç©º: {first_choice['message']}")

                return message_content

        except httpx.ConnectError as e:
            if "SSL" in str(e) or "EOF" in str(e):
                logger.warning(f"SSLè¿æ¥é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    import time
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                    continue
                else:
                    return f"SSLè¿æ¥å¤±è´¥: {str(e)}ã€‚å»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–é…ç½®ä»£ç†ã€‚"
            else:
                return f"è¿æ¥é”™è¯¯: {str(e)}"

        except httpx.TimeoutException as e:
            logger.warning(f"è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                import time
                time.sleep(1)
                continue
            else:
                return f"è¯·æ±‚è¶…æ—¶: {str(e)}"

        except httpx.HTTPStatusError as e:
            return f"HTTPé”™è¯¯ {e.response.status_code}: {e.response.text}"

        except Exception as e:
            logger.error(f"è°ƒç”¨DeepSeek APIæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            if attempt < max_retries - 1:
                import time
                time.sleep(1)
                continue
            else:
                return f"APIè°ƒç”¨å¤±è´¥: {str(e)}"
    return "APIè°ƒç”¨å¤±è´¥: è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°"

def _process_tool_calls(response: str, tools: Dict[str, Any]) -> str:
    """
    ã€å·²å®ç°ã€‘å¤„ç†AIå“åº”ä¸­å¯èƒ½åŒ…å«çš„å·¥å…·è°ƒç”¨ã€‚
    è§£æå¹¶æ‰§è¡Œå½¢å¦‚ `function_name(arg1, "arg2", ...)` çš„è°ƒç”¨ã€‚
    """
    response = response.strip()
    # ç§»é™¤å¯èƒ½çš„Markdownä»£ç å—æ ‡è®°
    if response.startswith("```") and response.endswith("```"):
        response = response.strip("`\n")
        if response.startswith("python"):
            response = response[6:].strip()

    # ä½¿ç”¨æ›´å¥å£®çš„æ­£åˆ™è¡¨è¾¾å¼æ¥åŒ¹é…å‡½æ•°è°ƒç”¨
    match = re.fullmatch(r"^\s*(\w+)\((.*)\)\s*$", response, re.DOTALL)
    if not match:
        # å¦‚æœä¸åŒ¹é…å·¥å…·è°ƒç”¨æ ¼å¼ï¼Œç›´æ¥è¿”å›AIçš„è‡ªç„¶è¯­è¨€å“åº”
        return response

    tool_name = match.group(1)
    args_str = match.group(2)

    if tool_name not in tools:
        # å¦‚æœAIå¹»è§‰å‡ºä¸€ä¸ªä¸å­˜åœ¨çš„å·¥å…·ï¼Œæˆ‘ä»¬ä¸åº”è¯¥æ‰§è¡Œå®ƒï¼Œè€Œæ˜¯è¿”å›åŸå§‹å“åº”
        logger.warning(f"AIè¯•å›¾è°ƒç”¨ä¸€ä¸ªä¸å­˜åœ¨çš„å·¥å…·: {tool_name}ã€‚è¿”å›åŸå§‹æ–‡æœ¬ã€‚")
        return response

    try:
        # ä½¿ç”¨ast.literal_evalå®‰å…¨åœ°è§£æå‚æ•°
        # å°è¯•è§£æä¸ºå…³é”®å­—å‚æ•°
        parsed_args = ()
        parsed_kwargs = {}
        if args_str.strip(): # ç¡®ä¿å‚æ•°å­—ç¬¦ä¸²ä¸ä¸ºç©º
            try:
                # å°è¯•åŒæ—¶è§£æä½ç½®å’Œå…³é”®å­—å‚æ•°
                # ä¸ºäº†å®‰å…¨ï¼Œæˆ‘ä»¬ç”¨ast.parseæ¥è§£æä¸€ä¸ªå‡½æ•°è°ƒç”¨è¡¨è¾¾å¼
                tree = ast.parse(f"f({args_str})", mode='eval')
                call_node = tree.body
                
                # è§£æä½ç½®å‚æ•°
                parsed_args = [ast.literal_eval(arg) for arg in call_node.args]

                # è§£æå…³é”®å­—å‚æ•°
                parsed_kwargs = {kw.arg: ast.literal_eval(kw.value) for kw in call_node.keywords}

            except (ValueError, SyntaxError, TypeError) as e:
                 logger.error(f"ä½¿ç”¨ASTè§£æå‚æ•° '{args_str}' å¤±è´¥: {e}ã€‚å°†ä½œä¸ºæ™®é€šæ–‡æœ¬å¤„ç†ã€‚")
                 return response # å‚æ•°è§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹AIå“åº”

        logger.info(f"æ‰§è¡Œå·¥å…·è°ƒç”¨: {tool_name} with args={parsed_args}, kwargs={parsed_kwargs}")
        tool_function = tools[tool_name]
        result = tool_function(*parsed_args, **parsed_kwargs)
        
        # å¯¹åˆ—è¡¨ç»“æœè¿›è¡Œæ ¼å¼åŒ–
        if isinstance(result, list):
            return "\n".join(map(str, result)) #ç¡®ä¿æ‰€æœ‰é¡¹éƒ½æ˜¯å­—ç¬¦ä¸²
        return str(result)

    except Exception as e:
        logger.error(f"è§£ææˆ–æ‰§è¡Œå·¥å…· '{tool_name}' æ—¶å‡ºé”™: {e}")
        return f"é”™è¯¯ï¼šæ‰§è¡Œå·¥å…· '{tool_name}' å¤±è´¥ã€‚åŸå› : {e}"

def run_agent_with_tools(agent: Dict, message: str) -> str:
    """
    ã€å·²ä¿®æ”¹ã€‘è¿è¡Œæ™ºèƒ½ä½“å¤„ç†æ¶ˆæ¯ã€‚
    ç§»é™¤äº†ç¡¬ç¼–ç çš„å…³é”®å­—åŒ¹é…ï¼Œè®©æ‰€æœ‰è¯·æ±‚éƒ½ç”±AIæ¨¡å‹å¤„ç†ã€‚
    """
    if not agent:
        return "æ™ºèƒ½ä½“æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚"
    try:
        full_prompt = f"{agent['system_prompt']}\n\nç”¨æˆ·: {message}\n\nåŠ©æ‰‹: "
        
        # 1. è®©AIå†³å®šæ˜¯ç›´æ¥å›ç­”è¿˜æ˜¯è°ƒç”¨å·¥å…·
        ai_response = _call_deepseek_api(full_prompt, agent.get('proxy_config'))
        
        # 2. å¤„ç†AIçš„å“åº”ï¼Œå¦‚æœå“åº”æ˜¯å·¥å…·è°ƒç”¨ï¼Œåˆ™æ‰§è¡Œå®ƒï¼›å¦åˆ™ç›´æ¥è¿”å›
        final_response = _process_tool_calls(ai_response, agent['tools'])
        
        return final_response

    except Exception as e:
        logger.error(f"æ™ºèƒ½ä½“å¤„ç†å¤±è´¥: {e}", exc_info=True)
        return f"æ™ºèƒ½ä½“å¤„ç†å¤±è´¥: {str(e)}"

# --- ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢ç«¯ç‚¹ (ç®€åŒ–) ---
# @app.get("/task/{task_id}") ...

# --- FastAPI è·¯ç”± ---
@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    redis_status = "disabled"
    if REDIS_AVAILABLE and redis_pool:
        try:
            redis_client = redis.Redis(connection_pool=redis_pool)
            await redis_client.ping()
            redis_status = "healthy"
        except Exception:
            redis_status = "error"
    
    return {
        "status": "healthy",
        "version": app.version, # ä½¿ç”¨app.version
        "features": {
            "redis": redis_status,
            "intelligent_agent": "enabled",
            "file_operations": "enabled",
            "network_search": "enabled" if tavily_api_key else "disabled",
            "ai_api": "enabled" if deepseek_api_key else "disabled"
        },
        "websocket_connections": len(manager.active_connections)
    }

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocketç«¯ç‚¹ï¼Œå¤„ç†å®æ—¶é€šä¿¡"""
    channel_id = await manager.connect(websocket)
    try:
        await manager.send_personal_message({
            "type": "connection",
            "data": {"status": "connected", "channel_id": channel_id},
            "timestamp": datetime.datetime.now().isoformat()
        }, channel_id)

        while True:
            data = await websocket.receive_json()
            if not isinstance(data, dict) or 'type' not in data:
                await manager.send_personal_message({"type": "error", "data": {"message": "æ— æ•ˆçš„æ¶ˆæ¯æ ¼å¼"}}, channel_id)
                continue

            message_type = data.get('type')
            if message_type == 'chat':
                await handle_chat_message(data, channel_id)
            elif message_type == 'ping':
                await manager.send_personal_message({"type": "pong"}, channel_id)
            else:
                await manager.send_personal_message({"type": "error", "data": {"message": f"ä¸æ”¯æŒçš„æ¶ˆæ¯ç±»å‹: {message_type}"}}, channel_id)

    except WebSocketDisconnect:
        logger.info(f"WebSocket {channel_id} æ–­å¼€è¿æ¥")
    except Exception as e:
        logger.error(f"WebSocket {channel_id} é”™è¯¯: {e}")
    finally:
        if channel_id:
            manager.disconnect(channel_id)

async def handle_chat_message(data: dict, channel_id: str):
    """å¤„ç†èŠå¤©æ¶ˆæ¯"""
    try:
        chat_data = data.get('data', {})
        message = chat_data.get('message', '').strip()
        if not message:
            await manager.send_personal_message({"type": "error", "data": {"message": "æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º"}}, channel_id)
            return

        await manager.send_personal_message({"type": "status", "data": {"status": "processing"}}, channel_id)

        task_data = {
            "message": message,
            "proxy_config": chat_data.get('proxy_config'),
        }

        agent = create_intelligent_agent(task_data.get('proxy_config'))
        response = run_agent_with_tools(agent, message)

        await manager.send_personal_message({
            "type": "result",
            "data": {"response": response, "success": True},
            "timestamp": datetime.datetime.now().isoformat()
        }, channel_id)

    except Exception as e:
        logger.error(f"å¤„ç†èŠå¤©æ¶ˆæ¯å¤±è´¥: {e}", exc_info=True)
        await manager.send_personal_message({"type": "error", "data": {"message": f"å¤„ç†å¤±è´¥: {str(e)}"}}, channel_id)

@app.post("/chat", response_model=ChatResponse, responses={400: {"model": ErrorResponse}, 500: {"model": ErrorResponse}})
async def chat(request: ChatRequest) -> ChatResponse:
    """èŠå¤©APIç«¯ç‚¹ (å…¼å®¹æ€§æ¥å£)"""
    user_message = request.message
    proxy_config = request.proxyConfig

    if not user_message.strip():
        raise HTTPException(status_code=400, detail="æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º")

    if proxy_config:
        is_valid, error_msg = validate_proxy_config(proxy_config)
        if not is_valid:
            raise HTTPException(status_code=400, detail=f"ä»£ç†é…ç½®æ— æ•ˆ: {error_msg}")

    try:
        logger.info(f"HTTPèŠå¤©è¯·æ±‚: {user_message}")
        proxy_config_dict = proxy_config.model_dump() if proxy_config else None
        agent = create_intelligent_agent(proxy_config_dict)
        response = run_agent_with_tools(agent, user_message)
        return ChatResponse(response=response)
    except Exception as e:
        logger.error(f"HTTPèŠå¤©å¤„ç†å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å¤„ç†è¯·æ±‚å¤±è´¥: {e}")

@app.post("/test-proxy", responses={400: {"model": ErrorResponse}, 500: {"model": ErrorResponse}})
async def test_proxy_endpoint(proxy_config: ProxyConfig):
    """æµ‹è¯•ä»£ç†è¿æ¥ç«¯ç‚¹"""
    is_valid, error_msg = validate_proxy_config(proxy_config)
    if not is_valid:
        raise HTTPException(status_code=400, detail=f"ä»£ç†é…ç½®æ— æ•ˆ: {error_msg}")
    try:
        success, message = await test_proxy_connection(proxy_config)
        return {"success": success, "message": message}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ä»£ç†æµ‹è¯•å¤±è´¥: {str(e)}")

def main():
    """ä¸»å‡½æ•° - å¯åŠ¨FastAPIæœåŠ¡"""
    logger.info("Chrome Plus V2.0 åç«¯æœåŠ¡å¯åŠ¨ä¸­...")
    os.makedirs(base_dir, exist_ok=True)
    
    import uvicorn
    from config import settings
    uvicorn.run(
        "__main__:app",
        host=settings.HOST,
        port=settings.PORT,
        log_level="info",
        reload=(ENVIRONMENT == 'development')
    )

if __name__ == "__main__":
    main()